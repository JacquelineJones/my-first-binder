{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c1a06b-e3c6-4c52-9b73-c40b74d80016",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 1\n",
    "\n",
    "#a What are the steps of kmeans?\n",
    "#Step 1: Assign each point to a cluster N at random\n",
    "#Step 2: Calculate the mean position of each cluster using the previous assignments\n",
    "#Step 3: Loop through the points - assign each point to the cluster whose center is closest\n",
    "#Step 4: Repeat this process until the centers stop moving around\n",
    "\n",
    "#b Create the builder function for step 1.\n",
    "label_randomly <- function(n_points, n_clusters){\n",
    "  sample(((1:n_points) %% n_clusters)+1, n_points, replace=F)\n",
    "}\n",
    "#c Create the builder function for step 2.\n",
    "get_cluster_means <- function(data, labels){\n",
    "  data %>%\n",
    "    mutate(label__ = labels) %>%\n",
    "    group_by(label__) %>%\n",
    "    summarize(across(everything(), mean), .groups = \"drop\") %>%\n",
    "    arrange(label__)\n",
    "}\n",
    "\n",
    "#d Create the builder function for step 3.\n",
    "assign_cluster_fast <- function(data, means){\n",
    "  data_matrix <- as.matrix(data)\n",
    "  means_matrix <- as.matrix(means %>% dplyr::select(-label__))\n",
    "  dii <- sort(rep(1:nrow(data), nrow(means)))\n",
    "  mii <- rep(1:nrow(means), nrow(data))\n",
    "  data_repped <- data_matrix[dii, ]\n",
    "  means_repped <- means_matrix[mii, ]\n",
    "  diff_squared <- (data_repped - means_repped)^2\n",
    "  all_distances <- rowSums(diff_squared)\n",
    "  tibble(dii=dii, mii=mii, distance=all_distances) %>%\n",
    "    group_by(dii) %>%\n",
    "    arrange(distance) %>%\n",
    "    filter(row_number()==1) %>%\n",
    "    ungroup() %>%\n",
    "    arrange(dii) %>%\n",
    "    pull(mii)\n",
    "}\n",
    "\n",
    "#e Create the builder function for step 4.\n",
    "kmeans_done <- function(old_means, new_means, eps=1e-6){\n",
    "  om <- as.matrix(old_means)\n",
    "  nm <- as.matrix(new_means)\n",
    "  m <- mean(sqrt(rowSums((om - nm)^2)))\n",
    "  if(m < eps) TRUE else FALSE\n",
    "\n",
    "#f Combine them all into your own kmeans function\n",
    "mykmeans <- function(data, n_clusters, eps=1e-6, max_it = 1000, verbose = FALSE){\n",
    "  labels <- label_randomly(nrow(data), n_clusters)\n",
    "  old_means <- get_cluster_means(data, labels)\n",
    "  done <- FALSE\n",
    "  it <- 0\n",
    "  while(!done & it < max_it){\n",
    "    labels <- assign_cluster_fast(data, old_means)\n",
    "    new_means <- get_cluster_means(data, labels)\n",
    "    if(kmeans_done(old_means, new_means)){\n",
    "      done <- TRUE\n",
    "    } else {\n",
    "      old_means <- new_means\n",
    "      it <- it + 1\n",
    "      if(verbose){\n",
    "        cat(sprintf(\"%d\\n\", it))\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "  list(labels=labels, means=new_means)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d32a8ce-8133-41b9-9350-46583aa0ce91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 2\n",
    "\n",
    "#a Read in the voltages_df.csv\n",
    "library(tidyverse)\n",
    "volt <- read_csv(\"voltages_df.csv\") \n",
    "\n",
    "#b Call your kmeans function with 3 clusters. Print the results with results$labels and results$means\n",
    "results <- mykmeans(volt, 3)\n",
    "print(results$labels)\n",
    "print(results$means)\n",
    "\n",
    "#c Call R's kmeans function with 3 clusters. Print the results with results$labels and results$cluster\n",
    "r_results <- kmeans(as.matrix(volt, 3)\n",
    "print(results$labels)\n",
    "print(results$cluster)\n",
    "    \n",
    "#d Are your labels/clusters the same? If not, why? Are your means the same?\n",
    "#Our labels/clusters are not the same; however, they are random and not indicative of differences in data. \n",
    "#Our means are slightly different, but primarily due to differences in the function. \n",
    "                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cadab22-6877-424d-96d5-372117e5487b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 3\n",
    "\n",
    "#a Explain the process of using a loop to assign clusters for kmeans\n",
    "#A for loop is a function that goes through the data points and constantly runs calculations to find the distance to each cluster center while picking the nearest cluster. \n",
    "\n",
    "#b Explain the process of vectorizing the code to assign clusters for kmeans\n",
    "#Vectorization uses a single operation on multiple data points at once.\n",
    "\n",
    "#c State which (for loops or vectorizing) is more efficient and why\n",
    "#Vectorizing is more efficient because is does not use loops; rather, it uses a single operation on multiple data points at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb500ce-a9d5-4664-ab4a-6f5b2b470b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 4\n",
    "\n",
    "#When does kmeans fail? What assumption does kmeans use that causes it to fail in this situation?\n",
    "#Kmeans can fail when the clusters are non-spherical, unqeual in size, or contain the presence of outliers. Kmeans assumes that clusters are spherical, that they are equal in size, and do not contain outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a074491e-19fc-46fb-84b0-ce9eb362319d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 5\n",
    "\n",
    "# What assumptions do Guassian mixture models make?\n",
    "They assume that the data is drawn from N Gaussian distributions whose individual parameters are estimated from the data. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5417653e-1d6a-4aa6-838c-fa56a5d4660b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 6\n",
    "\n",
    "#What assumption does spectral clustering make? Why does this help us? \n",
    "#Spectral clustering assumes that the clusters are defined by their connectivity rather than their compactness. This helps us because it is not held by the assumption that the data is non-spherical.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54f7910-7926-4ec1-b6d5-49f777b61ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 7\n",
    "\n",
    "#Define the gap statistic method. What do we use it for? The gap statistic method determines the optimal number of clusters in a dataset. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
