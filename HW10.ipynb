{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baca7d29-61d5-421e-b136-e96c89a33833",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 1\n",
    "Make a language model that uses ngrams and allows the user to specify start words, but uses a random start if one is not specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "534ebd47-8711-4b64-9c83-5f337e7b802d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#a Make a function to tokenize the text\n",
    "library(tokenizers)\n",
    "\n",
    "tokenize_text <- function(text) {\n",
    "  tokens <- tokenizers::tokenize_words(\n",
    "    text,\n",
    "    lowercase = TRUE,\n",
    "    strip_punct = TRUE\n",
    "  )[[1]]  \n",
    "  \n",
    "  return(tokens)\n",
    "}\n",
    "\n",
    "#b Make a function generate keys for ngrams\n",
    "key_from <- function(ngram, sep = \"\\x1f\") {\n",
    "    paste(ngram, collapse = sep)\n",
    "}\n",
    "\n",
    "\n",
    "#c Function to build an ngram table\n",
    "build_ngram_table <- function(tokens, n, sep = \"\\x1f\") {\n",
    "  if (length(tokens) < n) {\n",
    "    return(new.env(parent = emptyenv()))\n",
    "  }\n",
    "  \n",
    "  tbl <- new.env(parent = emptyenv())\n",
    "  \n",
    "\n",
    "  for (i in seq_len(length(tokens) - n + 1L)) {\n",
    "    ngram <- tokens[i:(i + n - 2L)]\n",
    "    next_word <- tokens[i + n - 1L]\n",
    "    key <- key_from(ngram, sep = sep)\n",
    "    counts <- if (!is.null(tbl[[key]])) tbl[[key]] else integer(0)\n",
    "    if (next_word %in% names(counts)) {\n",
    "      counts[[next_word]] <- counts[[next_word]] + 1L\n",
    "    } else {\n",
    "      counts[[next_word]] <- 1L\n",
    "    }\n",
    "    tbl[[key]] <- counts\n",
    "  }\n",
    "  \n",
    "  tbl\n",
    "}\n",
    "\n",
    "\n",
    "#d Function to digest the text\n",
    "digest_text <- function(text, n, sep = \"\\x1f\") {\n",
    "  tokens <- tokenize_text(text)\n",
    "  build_ngram_table(tokens, n = n, sep = sep)\n",
    "}\n",
    "\n",
    "\n",
    "#e Function to digest the url\n",
    "digest_url <- function(url, n, sep = \"\\x1f\") {\n",
    "  res <- httr::GET(url)\n",
    "  txt <- httr::content(res, as = \"text\", encoding = \"UTF-8\")\n",
    "  digest_text(txt, n = n, sep = sep)\n",
    "}\n",
    "\n",
    "\n",
    "#f Function that gives random start\n",
    "random_start <- function(tbl, sep = \"\\x1f\") {\n",
    "  keys <- ls(envir = tbl, all.names = TRUE)\n",
    "  if (length(keys) == 0L) {\n",
    "    stop(\"No n-grams available. Digest text first.\")\n",
    "  }\n",
    "  \n",
    "  picked <- sample(keys, 1)\n",
    "  strsplit(picked, sep, fixed = TRUE)[[1]]\n",
    "}\n",
    "\n",
    "#g Function to predict next word\n",
    "predict_next_word <- function(tbl, ngram, sep = \"\\x1f\") {\n",
    "  key <- key_from(ngram, sep = sep)\n",
    "  counts <- if (!is.null(tbl[[key]])) tbl[[key]] else integer(0)\n",
    "  \n",
    "  if (length(counts) == 0L) {\n",
    "    return(NA_character_)\n",
    "  }\n",
    "  \n",
    "  sample(\n",
    "    names(counts),\n",
    "    size = 1,\n",
    "    prob = as.numeric(counts)\n",
    "  )\n",
    "}\n",
    "\n",
    "#h  Function that puts everything together. Specify that if the user doesn't give a start word, then a random word will be used. \n",
    "make_ngram_generator <- function(tbl, n, sep = \"\\x1f\") {\n",
    "  force(tbl)\n",
    "  n <- as.integer(n)\n",
    "  force(sep)\n",
    "  \n",
    "  function(start_words = NULL, length = 10L) {\n",
    "    if (is.null(start_words) || length(start_words) != (n - 1L)) {\n",
    "      start_words <- random_start(tbl, sep = sep)\n",
    "    }\n",
    "    \n",
    "    word_sequence <- start_words\n",
    "    for (i in seq_len(max(0L, length - length(word_sequence)))) {\n",
    "      ngram <- tail(word_sequence, n - 1L)\n",
    "      next_word <- predict_next_word(tbl, ngram, sep = sep)\n",
    "      \n",
    "      if (is.na(next_word)) break\n",
    "      \n",
    "      word_sequence <- c(word_sequence, next_word)\n",
    "    }\n",
    "    \n",
    "    paste(word_sequence, collapse = \" \")\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26a12cbb-a1bc-42c9-a8bd-321ed2a9310b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in digest_url(grimms_url, n = n): could not find function \"digest_url\"\n",
     "output_type": "error",
     "traceback": [
      "Error in digest_url(grimms_url, n = n): could not find function \"digest_url\"\nTraceback:\n"
     ]
    }
   ],
   "source": [
    "#Question 2\n",
    "set.seed(2025)\n",
    "\n",
    "#a Test your model using text file of Grimm's Fairy Tales.\n",
    "grimms_url <- \"https://www.gutenberg.org/files/5314/5314-0.txt\"\n",
    "\n",
    "\n",
    "n <- 3\n",
    "\n",
    "# Digest Grimm's Fairy Tales into an n-gram table\n",
    "tbl3_grimm <- digest_url(grimms_url, n = n)\n",
    "\n",
    "# Make the generator\n",
    "gen3_grimm <- make_ngram_generator(tbl3_grimm, n = n)\n",
    "\n",
    "\n",
    "\n",
    "# i) Start words \"the king\", length 15\n",
    "output_i <- gen3_grimm(start_words = c(\"the\", \"king\"), length = 15)\n",
    "cat(\"i) n=3, start='the king', length=15:\\n\", output_i, \"\\n\\n\")\n",
    "\n",
    "output_ii <- gen3_grimm(length = 15)\n",
    "cat(\"ii) n=3, random start, length=15:\\n\", output_ii, \"\\n\")\n",
    "\n",
    "#Explain the difference in content generated by each source.\n",
    "#By specifying start words like, \"the king\", the model then begins to generate text from the specified context. However, when no \n",
    "#start words is given, the model will choose a n-gram. This means that the generated text will begin from a different\n",
    "#part and will produce content from potentially random parts of the story. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f44c71-d921-4bbf-8465-b20ea3c3f3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 3\n",
    "\n",
    "#a What is a language learning model?\n",
    "#A LLM is a type of machine learning model that predicts the probability of word sequences and has the capacity to generate human language. \n",
    "#LLM takes the previous input and then calculates the probability distribution over words. \n",
    "#There is also a version called the Markov model that predicts future states by considering the current state rather than previous input.\n",
    "\n",
    "#b How do you run a llm locally?\n",
    "#You can run a language learning model locally using a tool like Ollama. Ollama can be downloaded through Homebrew on the terminal and then will allow you to talk to the langauge\n",
    "#model over HTTP. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6babfa-7ad9-4fdb-9976-f5ffba553299",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 4\n",
    "Explain what the following vocab words mean in the context of typing mkdir project into the command line. If the term doesn't apply to this command, give the definition and/or an example.\n",
    "\n",
    "Term\t               Meaning\n",
    "Shell\tThe command mkdir project is processed through your shell\n",
    "Terminal emulator\tYou are entering the command mkdir project into the terminal emulator\n",
    "Process\tSomething running on your computer\n",
    "Signal\tThings we send to processes to tell them to do something\n",
    "Standard input\t(stdin) \"mkdir project\" is the stream of data that the project receives\n",
    "Standard output\t(stdout)The text confirming the creation of the directory\n",
    "Command line argument\t\"mkdir project\"\n",
    "The environment\tEverything that is visible while terminal is running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7488e840-5d12-4d35-8900-b8c01be1d097",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 5\n",
    "#Consider the following command find . -name \"*.R\" | xargs grep read_csv\n",
    "#a What are the programs?\n",
    "#find, xargs, and grep\n",
    "\n",
    "#b Explain what it's doing, step by step. \n",
    "#1. It starts in the current directory, searches, and then ouputs files ending in .R\n",
    "#2. Sends the list produced by finds\n",
    "#3. Xargs takes the file names and appends them to grep\n",
    "#4. Reads them to find any lines including read_csv\n",
    "#5. prints those"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3089bc-f2b2-4175-9a1d-414b72f90cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 6\n",
    "#Install docker \n",
    "\n",
    "#a Show the response when you run docker\n",
    "Unable to find image 'hello-world:latest' locally\n",
    "latest: Pulling from library/hello-world\n",
    "198f93fd5094: Pull complete \n",
    "Digest: sha256:f7931603f70e13dbd844253370742c4fc4202d290c80442b2e68706d8f33ce26\n",
    "Status: Downloaded newer image for hello-world:latest\n",
    "\n",
    "Hello from Docker!\n",
    "This message shows that your installation appears to be working correctly.\n",
    "\n",
    "To generate this message, Docker took the following steps:\n",
    " 1. The Docker client contacted the Docker daemon.\n",
    " 2. The Docker daemon pulled the \"hello-world\" image from the Docker Hub.\n",
    "    (arm64v8)\n",
    " 3. The Docker daemon created a new container from that image which runs the\n",
    "    executable that produces the output you are currently reading.\n",
    " 4. The Docker daemon streamed that output to the Docker client, which sent it\n",
    "    to your terminal.\n",
    "\n",
    "To try something more ambitious, you can run an Ubuntu container with:\n",
    " $ docker run -it ubuntu bash\n",
    "\n",
    "Share images, automate workflows, and more with a free Docker ID:\n",
    " https://hub.docker.com/\n",
    "\n",
    "For more examples and ideas, visit:\n",
    " https://docs.docker.com/get-started/\n",
    "\n",
    "#Access Rstudio through docker\n",
    "#How do you log in to the RStudio server?\n",
    "http://localhost:8787\n",
    "username: rstudio\n",
    "password: rstudio\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
